{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq_7JPYISQGH"
   },
   "source": [
    "https://lv99.tistory.com/17   참고\n",
    "https://github.com/SKTBrain/KoBERT\n",
    "참고(SKT의 한글 BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EoJWpYEWnN5",
    "outputId": "791499a7-1d42-4936-9ece-fad7419ae907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_2aCHGllfEU",
    "outputId": "023fe89e-5f38-467f-a1ae-00f56723de6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoBERT'...\n",
      "remote: Enumerating objects: 155, done.\u001b[K\n",
      "remote: Total 155 (delta 0), reused 0 (delta 0), pack-reused 155\u001b[K\n",
      "Receiving objects: 100% (155/155), 179.81 KiB | 5.14 MiB/s, done.\n",
      "Resolving deltas: 100% (82/82), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmQ__UMflup4",
    "outputId": "7212674d-f8fa-4eeb-b5fd-1aa91d7bfbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/KoBERT\n"
     ]
    }
   ],
   "source": [
    "cd KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tC2XwinlyB3",
    "outputId": "6eb3e11c-6af8-4639-fb8a-276683ff5fd0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.7.0+cu101)\n",
      "Collecting mxnet>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
      "\u001b[K     |████████████████████████████████| 55.0MB 76kB/s \n",
      "\u001b[?25hCollecting gluonnlp>=0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 44.9MB/s \n",
      "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 40.5MB/s \n",
      "\u001b[?25hCollecting onnxruntime>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/f009251fd1b91a2e1ce6f22d4b5be9936fbd0072842c5087a2a49706c509/onnxruntime-1.6.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 42.5MB/s \n",
      "\u001b[?25hCollecting transformers>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 34.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (1.19.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet>=1.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (0.29.21)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (20.8)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=0.3.0->-r requirements.txt (line 5)) (3.12.4)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 25.1MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 42.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (2019.12.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.6.0->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (51.0.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (1.0.0)\n",
      "Building wheels for collected packages: gluonnlp, sacremoses\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588522 sha256=802275bf91a24b4e61c5f7f68b4c4192611b925eb962dd3b7f9617531ab21c27\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=49fb40428e2dad5e66faa4eb6590f465cc0b28f4f4791a60b1c636e8651aa9b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built gluonnlp sacremoses\n",
      "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, onnxruntime, tokenizers, sacremoses, transformers\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 mxnet-1.7.0.post1 onnxruntime-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.4 transformers-4.1.1\n",
      "Processing /content/KoBERT\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kobert: filename=kobert-0.1.2-cp36-none-any.whl size=12734 sha256=0530111b49eb3d8018fa85edf1a13a3593821ebe02a28a4438f08d8dfb9b161f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fxfceebl/wheels/d0/ca/3e/bd91a227ac542246f4e8f94ff2f7f12a1a1f0c5b77601bcccf\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFUimldaR191",
    "outputId": "f39f6f4b-683a-4cf8-83a3-57fb5f2e7157",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.7.0.post1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.19.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.8)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-rsqM1QR7yP"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import gluonnlp as nlp\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# from kobert.utils import get_tokenizer\n",
    "# from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "# from transformers import AdamW\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irfxDKkQSAq_"
   },
   "outputs": [],
   "source": [
    "# ##GPU 사용 시\n",
    "# device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz7DfE81SBnB",
    "outputId": "d0b0c1d7-92ea-45e7-f2b7-52d2b97c6b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[██████████████████████████████████████████████████]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "tok_path = get_tokenizer()\n",
    "sp = SentencepieceTokenizer(tok_path)\n",
    "sp('한국어 모델을 공유합니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hg9MEz7CSCoQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jl5OVhUpXDPD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/content/drive/MyDrive/NLP/Dacon_NH competition/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### koBERT 형태소 분석용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te5RwTycOJjP",
    "outputId": "fe17ec5e-8713-4961-b4ad-4dbc1cfa08f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기: 8023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [▁[, 이, 데일리, ▁M, AR, K, E, T, P, O, IN, T, ], ...\n",
       "1         [▁\", 실적, 기, 반, \", ▁저, 가, 에, ▁매, 집, 해야, ▁할, ▁8,...\n",
       "2         [▁하이, 스, 탁, 론, ,, ▁선, 취, 수수료, ▁없는, ▁, 월, ▁, 0....\n",
       "3         [▁종합, ▁경제, 정보, ▁미디어, ▁이데일리, ▁-, ▁무, 단, 전, 재, ▁...\n",
       "4                 [▁전국, 적인, ▁소비, ▁, 붐, ▁조성, 에, ▁기여, 할, ▁예정]\n",
       "                                ...                        \n",
       "118740    [▁미, ▁F, D, A, ▁임, 상, 3, 상, ▁허가, ▁임, 박, ., ▁묻,...\n",
       "118741    [▁, 똑, 똑, 해진, ▁소비자, ., ., 한국, 도, ▁이, 젠, ▁소형, 차...\n",
       "118742    [▁, 똑, 똑, 해진, ▁소비자, ., ., 한국, 도, ▁이, 젠, ▁소형, 차...\n",
       "118743    [▁20, 20, 년, ▁한국, ▁TV, ▁2, 대, 중, ▁1, 대, ▁인터넷, ...\n",
       "118744    [▁20, 20, 년, ▁한국, ▁TV, ▁2, 대, 중, ▁1, 대, ▁인터넷, ...\n",
       "Name: content, Length: 118745, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/news_train.csv')\n",
    "test_data = pd.read_csv('data/news_test.csv')\n",
    "submission_data = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "x_train_data = train_data['content']\n",
    "y_train_data = train_data['info']\n",
    "x_test_data = test_data['content']\n",
    "\n",
    "# 콘텐츠 가져와서 형태소분석뒤 다시저장\n",
    "for i in range(0,len(x_train_data)):\n",
    "    x_train_data[i] = sp(x_train_data[i])\n",
    "\n",
    "#콘텐츠 가져와서 형태소분석뒤 다시저장\n",
    "for i in range(0,len(x_test_data)):\n",
    "    x_test_data[i] = sp(x_test_data[i])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_data)\n",
    "x_Train_sequences = tokenizer.texts_to_sequences(x_train_data)\n",
    "x_Test_sequences = tokenizer.texts_to_sequences(x_test_data)\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))\n",
    "\n",
    "vocabulary = tokenizer.word_index\n",
    "Train_data = x_Train_sequences\n",
    "Test_data = x_Test_sequences\n",
    "\n",
    "max_len = 25\n",
    "X_train = pad_sequences(Train_data, maxlen=max_len)\n",
    "y_train = y_train_data\n",
    "X_test = pad_sequences(Test_data, maxlen=max_len)\n",
    "\n",
    "x_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### koBERT + Mecab 형태소 분석용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTdC86KhSEc9",
    "outputId": "4250ae3c-8a2d-4d2e-a1b7-44d0b6f86cbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기: 5956\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/news_train_preprocessing_Mecab(all_alphabet)_12.21.csv')\n",
    "test_data = pd.read_csv('data/news_test_preprocessing_Mecab(all_alphabet)_12.21.csv')\n",
    "submission_data = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "train_data = train_data.fillna('')\n",
    "test_data = test_data.fillna('')\n",
    "\n",
    "x_train_data = train_data['text']\n",
    "y_train_data = train_data['info']\n",
    "x_test_data = test_data['text']\n",
    "\n",
    "# 콘텐츠 가져와서 형태소분석뒤 다시저장\n",
    "for i in range(0,len(x_train_data)):\n",
    "    x_train_data[i] = sp(x_train_data[i])\n",
    "\n",
    "#콘텐츠 가져와서 형태소분석뒤 다시저장\n",
    "for i in range(0,len(x_test_data)):\n",
    "    x_test_data[i] = sp(x_test_data[i])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_data)\n",
    "x_Train_sequences = tokenizer.texts_to_sequences(x_train_data)\n",
    "x_Test_sequences = tokenizer.texts_to_sequences(x_test_data)\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))\n",
    "\n",
    "vocabulary = tokenizer.word_index\n",
    "Train_data = x_Train_sequences\n",
    "Test_data = x_Test_sequences\n",
    "\n",
    "max_len = 25\n",
    "X_train = pad_sequences(Train_data, maxlen=max_len)\n",
    "y_train = y_train_data\n",
    "X_test = pad_sequences(Test_data, maxlen=max_len)\n",
    "\n",
    "x_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "bZLjH_CYuEdc",
    "outputId": "3caa3aa9-1191-4f55-ee30-4965a11929c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118745.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.097267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.316691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2082.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token_length\n",
       "count  118745.000000\n",
       "mean       30.097267\n",
       "std        24.316691\n",
       "min         0.000000\n",
       "25%        14.000000\n",
       "50%        22.000000\n",
       "75%        39.000000\n",
       "max      2082.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length = [len(l) for l in x_train_data]\n",
    "temp_df = pd.DataFrame(token_length, columns = ['token_length'])\n",
    "temp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv-tr1fWuGdX",
    "outputId": "67a380a0-fd6a-4b37-a0f9-7ca0570cf572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(temp_df['token_length'], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zo-4oXaWUwsO",
    "outputId": "620ac0ec-2298-41d4-ada8-b7ac114a4ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150, 5, 959, 992, 1014, 122, 144, 199, 36, 63, 843, 199, 128, 725, 934, 2944, 350, 1055, 656, 218, 2271, 251, 1396], [29, 175, 42, 399, 24, 97, 6, 8, 162, 75, 392, 410, 299, 53, 267, 169, 435, 16, 120, 63, 36, 115, 71, 376, 95], [739, 44, 110, 123, 3, 321, 506, 542, 540, 1, 53, 1, 903, 129, 132, 505, 521], [474, 308, 531, 774, 851, 398, 204, 121, 89, 104, 213, 160, 66, 143, 317], [721, 119, 1369, 1, 6210, 1074, 8, 1684, 40, 838]]\n",
      "[[266, 5, 368, 9, 1, 2954, 227, 2107, 502, 190, 4832, 166, 71, 96, 968, 103, 426, 9, 943, 393, 11, 4877, 334, 19, 109, 88, 17, 6, 94], [150, 5, 959, 336, 25, 235, 1, 140, 128, 1506, 2107, 1146, 54, 108, 44, 18, 2230, 2961, 1115, 12, 7, 528, 2167, 266, 5, 368, 9, 515, 1046, 37, 5, 292, 126, 18, 2954, 12, 226, 10, 2705, 956, 3428, 1073], [312, 1661, 3492, 26, 433, 10, 3017, 17, 173, 2107, 623, 502, 190, 8, 4832, 640, 487, 11, 347, 19, 1409, 6, 10, 2251, 17, 1073], [227, 10, 1801, 605, 9, 5265, 266, 5, 368, 9, 1, 2954, 7, 226, 6, 3849, 524, 118, 40, 165, 403, 1322, 2042, 6, 1141, 3890, 2450, 1122, 6, 1166, 486, 3804, 1144], [54, 108, 1328, 1506, 2107, 10, 1781, 3, 546, 55, 1146, 9, 280, 6422, 23, 440, 53, 1055, 2158, 935, 805, 3, 2163, 16, 22, 481, 100, 857, 628, 6, 3168, 1864, 22, 26, 145, 1757, 115, 3738, 792, 10, 1833, 472, 3710, 9, 1073]]\n"
     ]
    }
   ],
   "source": [
    "print(x_Train_sequences[:5])\n",
    "print(x_Test_sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText 적용 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiJQZBzoyf_K",
    "outputId": "6626cbb2-98e8-4694-b93f-99b00ab9a9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fastText' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WnfuZ4uytFV"
   },
   "outputs": [],
   "source": [
    "cd fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ft = pd.read_csv('data/news_train.csv')\n",
    "x_train_data_ft = train_data_ft['content']\n",
    "x_train_data_ft.to_csv('train_content.txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGMgFHERXXDQ"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import gensim\n",
    "\n",
    "path = '/content/drive/MyDrive/NLP/Dacon_NH competition/train_content.txt'\n",
    "sentence = gensim.models.word2vec.Text8Corpus(path)\n",
    "\n",
    "model = FastText(sentence, size=100, window=5, sg=1, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8TzPpsAXb7b",
    "outputId": "a6ac160f-67aa-4c1f-a054-806af2fe79b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))     # 100차원의 임베딩 매트릭스 생성\n",
    "cnt = 0\n",
    "\n",
    "for index, word in enumerate(vocabulary):          # vocabulary에 있는 토큰들을 하나씩 넘김\n",
    "    if word in model:                                # 넘겨받은 토큰이 FastText에 존재하면(이미 훈련이 된 토큰이라는 의미)\n",
    "        embedding_vector = model[word]                 # 해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_matrix[index] = embedding_vector     # 해당 위치의 embedding_matrix에 저장\n",
    "    else:\n",
    "        cnt += 1\n",
    "        # print(\"FastText에 없는 단어입니다.\")\n",
    "        continue\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim Word2Vec 적용 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWa3kq6ksbAz",
    "outputId": "0fc8adca-fbe2-402c-f0ae-5c5b921ad0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))         # 300차원의 임베딩 매트릭스 생성\n",
    "cnt = 0\n",
    "\n",
    "for index, word in enumerate(vocabulary):              # vocabulary에 있는 토큰들을 하나씩 넘김\n",
    "    if word in word2vec:                               # 넘겨받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 의미)\n",
    "        embedding_vector = word2vec[word]              # 해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_matrix[index] = embedding_vector     # 해당 위치의 embedding_matrix에 저장\n",
    "    else:\n",
    "        cnt += 1\n",
    "        # print(\"word2vec에 없는 단어입니다.\")\n",
    "        continue\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim Word2Vec Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sne60PZaCPuw",
    "outputId": "e43d6c7a-00b4-483d-fadd-6d53ddd645eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 300)           1786800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 25, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,830,513\n",
      "Trainable params: 43,713\n",
      "Non-trainable params: 1,786,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "1856/1856 [==============================] - 38s 19ms/step - loss: 0.3185 - acc: 0.8522\n",
      "Epoch 2/7\n",
      "1856/1856 [==============================] - 35s 19ms/step - loss: 0.2122 - acc: 0.9103\n",
      "Epoch 3/7\n",
      "1856/1856 [==============================] - 36s 19ms/step - loss: 0.1997 - acc: 0.9149\n",
      "Epoch 4/7\n",
      "1856/1856 [==============================] - 36s 19ms/step - loss: 0.1912 - acc: 0.9174\n",
      "Epoch 5/7\n",
      "1856/1856 [==============================] - 35s 19ms/step - loss: 0.1905 - acc: 0.9170\n",
      "Epoch 6/7\n",
      "1856/1856 [==============================] - 35s 19ms/step - loss: 0.1916 - acc: 0.9171\n",
      "Epoch 7/7\n",
      "1856/1856 [==============================] - 36s 19ms/step - loss: 0.1876 - acc: 0.9186\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "embedding_dim = 300\n",
    "max_len = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_len))      # 임베딩 가중치 적용 코드\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.0001)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])   # 임베딩 층에 사전 훈련된 임베딩값 주입\n",
    "model.layers[0].trainable = False                 # 그리고 이 임베딩 층은 훈련되지 못하도록 동결\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.01, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=7, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BG9SMNcdsvgg",
    "outputId": "b50c55c7-e59e-4502-f59f-d6e04f9ed52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 50)            401150    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 25, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 412,047\n",
      "Trainable params: 412,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n",
      "3711/3711 [==============================] - 52s 13ms/step - loss: 0.1677 - acc: 0.9457\n",
      "Epoch 2/4\n",
      "3711/3711 [==============================] - 49s 13ms/step - loss: 0.0612 - acc: 0.9791\n",
      "Epoch 3/4\n",
      "3711/3711 [==============================] - 49s 13ms/step - loss: 0.0519 - acc: 0.9819\n",
      "Epoch 4/4\n",
      "3711/3711 [==============================] - 49s 13ms/step - loss: 0.0475 - acc: 0.9841\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "embedding_dim = 50      # 50 ~ 200 사이에서 적절히 설정\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim = embedding_dim, input_length = max_len))   # vocab_size : 8023\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNBh4btFsmb2",
    "outputId": "087e2654-6676-478d-9dbc-a6038b9fafa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 100)           802300    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 819,597\n",
      "Trainable params: 17,297\n",
      "Non-trainable params: 802,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n",
      "1856/1856 [==============================] - 27s 13ms/step - loss: 0.3398 - acc: 0.8498\n",
      "Epoch 2/4\n",
      "1856/1856 [==============================] - 25s 14ms/step - loss: 0.1524 - acc: 0.9463\n",
      "Epoch 3/4\n",
      "1856/1856 [==============================] - 25s 14ms/step - loss: 0.1263 - acc: 0.9554\n",
      "Epoch 4/4\n",
      "1856/1856 [==============================] - 25s 14ms/step - loss: 0.1161 - acc: 0.9585\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length = max_len))      # 임베딩 가중치 적용 코드\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer = regularizers.l2(0.0001)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])   # 임베딩 층에 사전 훈련된 임베딩값 주입\n",
    "model.layers[0].trainable = False                 # 그리고 이 임베딩 층은 훈련되지 못하도록 동결\n",
    "\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0003, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "bHoJ6l2It45F",
    "outputId": "1f60a299-092d-4f22-d8ac-10f1164b4e0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS00237_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS00237_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS00237_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS00237_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS00237_5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142560</th>\n",
       "      <td>NEWS09482_72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142561</th>\n",
       "      <td>NEWS09482_73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142562</th>\n",
       "      <td>NEWS09482_74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142563</th>\n",
       "      <td>NEWS09482_75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142564</th>\n",
       "      <td>NEWS09482_76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142565 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  info\n",
       "0        NEWS00237_1     1\n",
       "1        NEWS00237_2     0\n",
       "2        NEWS00237_3     0\n",
       "3        NEWS00237_4     0\n",
       "4        NEWS00237_5     0\n",
       "...              ...   ...\n",
       "142560  NEWS09482_72     1\n",
       "142561  NEWS09482_73     1\n",
       "142562  NEWS09482_74     1\n",
       "142563  NEWS09482_75     1\n",
       "142564  NEWS09482_76     1\n",
       "\n",
       "[142565 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "submission_data.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "\n",
    "submission_data.loc[:,[\"id\",\"info\"]].to_csv(\"submission_data_Mecab_KoBERT(gensim)_wandb_12_31.csv\", index = False)\n",
    "submission_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzKreExNbD7n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "koBERT_FastText_1229.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
