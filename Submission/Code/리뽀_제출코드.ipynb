{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22729,
     "status": "ok",
     "timestamp": 1610012148911,
     "user": {
      "displayName": "이찬솔",
      "photoUrl": "",
      "userId": "11969419735175061289"
     },
     "user_tz": -540
    },
    "id": "2m4NZ-AElueJ",
    "outputId": "8b32e9f2-7616-4d94-f948-e8e7a2adc7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1610012152552,
     "user": {
      "displayName": "이찬솔",
      "photoUrl": "",
      "userId": "11969419735175061289"
     },
     "user_tz": -540
    },
    "id": "8QLLmoxcw4nK",
    "outputId": "f6a0507e-1a0a-4526-e6ca-81715daac047"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2325,
     "status": "ok",
     "timestamp": 1610012157526,
     "user": {
      "displayName": "이찬솔",
      "photoUrl": "",
      "userId": "11969419735175061289"
     },
     "user_tz": -540
    },
    "id": "9_2aCHGllfEU",
    "outputId": "46c9f2d1-b777-4c5b-b93d-21f5cb8c0e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoBERT'...\n",
      "remote: Enumerating objects: 155, done.\u001b[K\n",
      "remote: Total 155 (delta 0), reused 0 (delta 0), pack-reused 155\u001b[K\n",
      "Receiving objects: 100% (155/155), 179.81 KiB | 821.00 KiB/s, done.\n",
      "Resolving deltas: 100% (82/82), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1610012158900,
     "user": {
      "displayName": "이찬솔",
      "photoUrl": "",
      "userId": "11969419735175061289"
     },
     "user_tz": -540
    },
    "id": "zmQ__UMflup4",
    "outputId": "ee58a278-3728-4696-d1de-92c3dbd78f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/KoBERT\n"
     ]
    }
   ],
   "source": [
    "cd KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26385,
     "status": "ok",
     "timestamp": 1610012185050,
     "user": {
      "displayName": "이찬솔",
      "photoUrl": "",
      "userId": "11969419735175061289"
     },
     "user_tz": -540
    },
    "id": "6tC2XwinlyB3",
    "outputId": "d7fd7fb0-5161-4722-adaa-cf91bb685916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.7.0+cu101)\n",
      "Collecting mxnet>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
      "\u001b[K     |████████████████████████████████| 55.0MB 73kB/s \n",
      "\u001b[?25hCollecting gluonnlp>=0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 63.9MB/s \n",
      "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 41.8MB/s \n",
      "\u001b[?25hCollecting onnxruntime>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/f009251fd1b91a2e1ce6f22d4b5be9936fbd0072842c5087a2a49706c509/onnxruntime-1.6.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 44.2MB/s \n",
      "\u001b[?25hCollecting transformers>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 41.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (1.19.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet>=1.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (0.29.21)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (20.8)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=0.3.0->-r requirements.txt (line 5)) (3.12.4)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 38.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 48.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.6.0->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (51.0.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (1.0.0)\n",
      "Building wheels for collected packages: gluonnlp, sacremoses\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588507 sha256=d37d188caae26278933a63a4daac734967351f582153424626645876d61b6273\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=041ab04a74ef38bf19cf9b895a72d15f3e89dbbb650d5af4abe010f08941eecf\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built gluonnlp sacremoses\n",
      "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, onnxruntime, tokenizers, sacremoses, transformers\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 mxnet-1.7.0.post1 onnxruntime-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.4 transformers-4.1.1\n",
      "Processing /content/KoBERT\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kobert: filename=kobert-0.1.2-cp36-none-any.whl size=12734 sha256=fde515009f5269e4ab5ca26c9881d7699cd81c46b2dfc9729d8d7735d73850b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cky7e1s8/wheels/d0/ca/3e/bd91a227ac542246f4e8f94ff2f7f12a1a1f0c5b77601bcccf\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFUimldaR191",
    "outputId": "481b948a-3e2b-44e9-911a-2fe179e035cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3fmuRHNs72J"
   },
   "outputs": [],
   "source": [
    "# 모듈 임포트\n",
    "\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  \n",
    "import os\n",
    "import pickle\n",
    "import gensim\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz7DfE81SBnB",
    "outputId": "e1f77799-494b-42e1-dd21-14b0f9806d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "tok_path = get_tokenizer()\n",
    "sp = SentencepieceTokenizer(tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3U2s6H1mDzq"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/리뽀/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pksJuaskdAO",
    "outputId": "f6d75e28-d852-4574-dc83-c519b0bb8178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('1. Data/news_train.csv')\n",
    "\n",
    "x_train_data = train_data['content']\n",
    "y_train_data = train_data['info']\n",
    "\n",
    "# 콘텐츠 가져와서 형태소분석뒤 다시저장\n",
    "for i in range(0, len(x_train_data)):\n",
    "  x_train_data[i] = sp(x_train_data[i])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_data)\n",
    "x_Train_sequences = tokenizer.texts_to_sequences(x_train_data)\n",
    "\n",
    "vocabulary = tokenizer.word_index\n",
    "vocab_size = len(vocabulary) + 1\n",
    "\n",
    "Train_data = x_Train_sequences\n",
    "\n",
    "max_len = 25\n",
    "X_train = pad_sequences(Train_data, maxlen = max_len)\n",
    "y_train = y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qN7DDTMnfgp"
   },
   "outputs": [],
   "source": [
    "# 토크나이저 저장\n",
    "\n",
    "with open('3. Tokenizer/tokenizer.pickle', 'wb') as handle:  \n",
    "  pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWa3kq6ksbAz"
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('4. Pre_trained embedding/GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))     # 300차원의 임베딩 매트릭스 생성\n",
    "cnt = 0\n",
    "\n",
    "for index, word in enumerate(vocabulary):          # vocabulary에 있는 토큰들을 하나씩 넘김\n",
    "  if word in word2vec:                             # 넘겨받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 의미)\n",
    "    embedding_vector = word2vec[word]              # 해당 토큰에 해당하는 vector를 불러오고\n",
    "    embedding_matrix[index] = embedding_vector     # 해당 위치의 embedding_matrix에 저장\n",
    "  else:\n",
    "    cnt += 1\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUbb3fJqy8xB"
   },
   "outputs": [],
   "source": [
    "np.save('4. Pre_trained embedding/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNBh4btFsmb2",
    "outputId": "c8761244-079b-4eb5-cd88-043eb8f2c33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           2406900   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 25, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,450,613\n",
      "Trainable params: 2,450,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "3711/3711 [==============================] - 109s 29ms/step - loss: 0.1076 - accuracy: 0.9584\n"
     ]
    }
   ],
   "source": [
    "max_len = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_len))  # 임베딩 가중치 적용 코드\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.0001)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics='accuracy')\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXHGAE0hnX1H"
   },
   "outputs": [],
   "source": [
    "model.save('5. Model/kobert_gensim_model.h5')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq7wiLxAeKgb"
   },
   "source": [
    "Test 시간재기 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ro3bP1iciGIN"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1rF9GJGia-f"
   },
   "outputs": [],
   "source": [
    "cd KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iugZjKZdid1m"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmKyY4VbijT1"
   },
   "outputs": [],
   "source": [
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVUeUQIwvNSn"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('1. Data/news_test.csv')\n",
    "submission_data = pd.read_csv('1. Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aBbXPhxe7sE"
   },
   "outputs": [],
   "source": [
    "# 시간 측정 시작\n",
    "\n",
    "import time\n",
    "start = time.time()  # 시작 시간 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qAnL5CGvIVr"
   },
   "outputs": [],
   "source": [
    "# Library 불러오기\n",
    "\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  \n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulCiGy4Cn32v",
    "outputId": "81e38fc7-2d92-4986-b127-e736ed65e6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# pos_tagger, tokenizer, pretrained_embedding, model 불러오기\n",
    "\n",
    "tok_path = get_tokenizer()\n",
    "sp = SentencepieceTokenizer(tok_path)                              # pos_tagger\n",
    "\n",
    "with open('3. Tokenizer/tokenizer.pickle', 'rb') as handle:        # tokenizer\n",
    "       loaded_tokenizer = pickle.load(handle)\n",
    "\n",
    "np.load('4. Pre_trained embedding/embedding_matrix.npy')           # pre_trained embedding\n",
    "\n",
    "loaded_model = load_model('5. Model/kobert_gensim_model.h5')       # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytTO2AMNoPJR",
    "outputId": "c053ad8d-d900-4fae-82d3-23b07071ed8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 + 전처리\n",
    "\n",
    "x_test_data = test_data['content']\n",
    "\n",
    "for i in range(0, len(x_test_data)):\n",
    "  x_test_data[i] = sp(x_test_data[i])\n",
    "  \n",
    "tokenizer = loaded_tokenizer\n",
    "x_Test_sequences = tokenizer.texts_to_sequences(x_test_data)\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index) + 1\n",
    "\n",
    "vocabulary = tokenizer.word_index\n",
    "Test_data = x_Test_sequences\n",
    "\n",
    "max_len = 25\n",
    "X_test = pad_sequences(Test_data, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvIYqpWGuwRO"
   },
   "outputs": [],
   "source": [
    "# 예측\n",
    "\n",
    "pred_test = loaded_model.predict(X_test)\n",
    "submission_data.loc[:,'info'] = np.where(pred_test > 0.5, 1,0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtXs1_tVe_o4",
    "outputId": "dfaa59c8-1ec8-4cdf-fc74-2653a1728810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 263.28223943710327\n"
     ]
    }
   ],
   "source": [
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "_I6lwQ8Pvz48",
    "outputId": "649e41b9-b93a-41ee-8d7a-1a6fca1b401a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS00237_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS00237_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS00237_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS00237_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS00237_5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142560</th>\n",
       "      <td>NEWS09482_72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142561</th>\n",
       "      <td>NEWS09482_73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142562</th>\n",
       "      <td>NEWS09482_74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142563</th>\n",
       "      <td>NEWS09482_75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142564</th>\n",
       "      <td>NEWS09482_76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142565 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  info\n",
       "0        NEWS00237_1     1\n",
       "1        NEWS00237_2     0\n",
       "2        NEWS00237_3     0\n",
       "3        NEWS00237_4     0\n",
       "4        NEWS00237_5     0\n",
       "...              ...   ...\n",
       "142560  NEWS09482_72     1\n",
       "142561  NEWS09482_73     1\n",
       "142562  NEWS09482_74     1\n",
       "142563  NEWS09482_75     1\n",
       "142564  NEWS09482_76     1\n",
       "\n",
       "[142565 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제출\n",
    "\n",
    "submission_data.loc[:, [\"id\",\"info\"]].to_csv(\"submission_data(KoBERT)(gensim)1228(noval)ep1-newpara.csv\", index = False)\n",
    "submission_data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "리뽀_제출코드.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
